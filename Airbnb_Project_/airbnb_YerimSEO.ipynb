{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Setting up your environment\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "# The following packages will also be used in this tutorial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Getting started\n",
    "\"\"\"\n",
    "def getPage(url):\n",
    "    ''' returns a soup object that contains all the information \n",
    "    of a certain webpage'''\n",
    "    result = requests.get(url)\n",
    "    content = result.content\n",
    "    return BeautifulSoup(content, features = \"lxml\")\n",
    "\n",
    "    \n",
    "def getRoomClasses(soupPage):\n",
    "    ''' This function returns all the listings that can \n",
    "    be found on the page in a list.'''\n",
    "    rooms = soupPage.findAll(\"div\", {\"class\": \"_8ssblpx\"})\n",
    "    result = []\n",
    "    for room in rooms:\n",
    "        result.append(room)\n",
    "    return result\n",
    "\n",
    "def getListingLink(listing):\n",
    "    ''' This function returns the link of the listing'''\n",
    "    return \"http://airbnb.com\" + listing.find(\"a\")[\"href\"]\n",
    "\n",
    "def getListingTitle(listing):\n",
    "    ''' This function returns the title of the listing'''\n",
    "    return listing.find(\"meta\")[\"content\"]\n",
    "\n",
    "def getTopRow(listing):\n",
    "    ''' Returns the top row of listing information'''\n",
    "    return listing.find(\"div\", {\"class\": \"_167qordg\"}).text\n",
    "\n",
    "def getRoomInfo(listing):\n",
    "    ''' Returns the guest information'''\n",
    "    return listing.find(\"div\", {\"class\":\"_kqh46o\"}).text\n",
    "\n",
    "def getBasicFacilities(listing):\n",
    "    ''' Returns the basic facilities'''\n",
    "    try:\n",
    "        output = listing.findAll(\"div\", {\"class\":\"_kqh46o\"})[1].text.replace(\" \",\"\") #Speeds up cleaning\n",
    "    except:\n",
    "        output = []\n",
    "    return output\n",
    "\n",
    "def getListingPrice(listing):\n",
    "    ''' Returns the price'''\n",
    "    return listing.find(\"div\", {\"class\":\"_1fwiw8gv\"}).text\n",
    "\n",
    "def getListingRating(listing):\n",
    "    ''' Returns the rating '''\n",
    "    return listing.find(\"span\", {\"class\":\"_krjbj\"}).text\n",
    "\n",
    "def getListingReviewNumber(listing):\n",
    "    ''' Returns the number of reviews '''\n",
    "    try: # Not all listings have reviews // extraction failed\n",
    "        output = listing.findAll(\"span\", {\"class\":\"_krjbj\"})[1].text\n",
    "    except:\n",
    "        output = -1   # Indicate that the extraction failed -> can indicate no reviews or a mistake in scraping\n",
    "    return output\n",
    "\n",
    "def extractInformation(soupPage):\n",
    "    ''' Takes all the information of a single page (thus multiple listings) and\n",
    "    summarizes it in a dataframe'''\n",
    "    listings = getRoomClasses(soupPage)\n",
    "    titles, links, toprows, roominfos, basicfacilitiess, prices, ratings, reviews = [], [], [], [], [], [], [], []\n",
    "    for listing in listings:\n",
    "        titles.append(getListingTitle(listing))\n",
    "        links.append(getListingLink(listing))\n",
    "        toprows.append(getTopRow(listing))\n",
    "        roominfos.append(getRoomInfo(listing))\n",
    "        basicfacilitiess.append(getBasicFacilities(listing))\n",
    "        prices.append(getListingPrice(listing))\n",
    "        ratings.append(getListingRating(listing))\n",
    "        reviews.append(getListingReviewNumber(listing))\n",
    "    dictionary = {\"title\": titles, \"toprow\": toprows, \"roominfo\": roominfos, \"facilities\" : basicfacilitiess, \"price\": prices, \"rating\": ratings, \"link\": links, \"reviewnumber\": reviews}\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "'''\n",
    "    Scraping all listings for a given city\n",
    "'''\n",
    "def findNextPage(soupPage):\n",
    "    ''' Finds the next page with listings if it exists '''\n",
    "    try:\n",
    "        nextpage = \"https://airbnb.com\" + soupPage.find(\"li\", {\"class\": \"_i66xk8d\"}).find(\"a\")[\"href\"]\n",
    "    except:\n",
    "        nextpage = \"no next page\"\n",
    "    return nextpage\n",
    "\n",
    "def getPages(url):\n",
    "    ''' This function returns all the links to the pages containing \n",
    "    listings for one particular city '''\n",
    "    result = []\n",
    "    while url != \"no next page\": \n",
    "        page = getPage(url)\n",
    "        result = result + [page]\n",
    "        url = findNextPage(page)\n",
    "    return result\n",
    "\n",
    "def extractPages(url):\n",
    "    ''' This function outputs a dataframe that contains all information of a particular\n",
    "    city. It thus contains information of multiple listings coming from multiple pages.'''\n",
    "    pages = getPages(url)\n",
    "    # Do for the first element to initialize the dataframe\n",
    "    df = extractInformation(pages[0])\n",
    "    # Loop over all other elements of the dataframe\n",
    "    for pagenumber in range(1, len(pages)):\n",
    "        df = df.append(extractInformation(pages[pagenumber]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrapeURLs(listofURLs):\n",
    "    ''' This function scrapes all listings of the cities listed in a list together\n",
    "    with their URLs'''\n",
    "    print(listofURLs[0][0]) # Shows which city is being scraped\n",
    "    # Do it for the first element in the list to initialize dataframe\n",
    "    df = extractPages(listofURLs[0][1])\n",
    "    df.loc[:, \"city\"] = listofURLs[0][0] # Add the city as a feature\n",
    "    # loop over all the other elements in the list and append to dataframe\n",
    "    for i in range(1, len(listofURLs)):\n",
    "        print(listofURLs[i][0]) # Shows which city is being scraped\n",
    "        newrows = extractPages(listofURLs[i][1])\n",
    "        newrows.loc[:, \"city\"] = listofURLs[i][0] # Add the city as a feature\n",
    "        df = df.append(newrows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Scraping detailed information of rooms with beautifulsoup\n",
    "'''\n",
    "\n",
    "def getDescription(detailpage):\n",
    "    ''' Returns the self written description of the host '''\n",
    "    return detailpage.find(\"div\", {\"class\": \"_eeq7h0\"}).text\n",
    "\n",
    "def getDetailedScores(detailpage):\n",
    "    output = []\n",
    "    scores = detailpage.findAll(class_ = '_a3qxec')\n",
    "    try: # sometimes a listing does not have any reviews\n",
    "        for i in range(0, 6):\n",
    "            split = scores[i].text.split(\".\")\n",
    "            output.append(float(split[0][-1] + \".\" + split[1]))\n",
    "    except: # then we just don't want to pass any scores\n",
    "        pass\n",
    "    return output\n",
    "\n",
    "def getHostInfo(detailpage):\n",
    "    ''' Returns the name of the host and when they joined'''\n",
    "    return detailpage.find(class_ = \"_f47qa6\").text\n",
    "\n",
    "\n",
    "'''\n",
    "    Using selenium for all other information\n",
    "'''\n",
    "def setupDriver(url, waiting_time = 2.5):\n",
    "    ''' Initializes the driver of selenium'''\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(waiting_time) \n",
    "    return driver\n",
    "\n",
    "\n",
    "def getJSpage(url):\n",
    "    ''' Extracts the html of the webpage including the JS elements,\n",
    "    output should be used as the input for all functions extracting specific information\n",
    "    from the detailed pages of the listings '''\n",
    "    driver = setupDriver(url)\n",
    "    read_more_buttons = driver.find_elements_by_class_name(\"_1d079j1e\")\n",
    "    try:\n",
    "        for i in range(2, len(read_more_buttons)):\n",
    "            read_more_buttons[i].click()\n",
    "    except:\n",
    "        pass\n",
    "    html = driver.page_source\n",
    "    driver.close()\n",
    "    return BeautifulSoup(html, features=\"lxml\") \n",
    "\n",
    "\n",
    "def getAmenitiesPage(detailpage):\n",
    "    ''' This code fetches the html of the webpage containing the information\n",
    "     about the amenities that are available in the room'''\n",
    "    \n",
    "    link = detailpage.find(class_ = \"_1v4ygly5\")[\"href\"]\n",
    "    driver = setupDriver(\"https://airbnb.com\" + link, 5) # Amenitiespage is a link disguished as a button, this is why I need to do this\n",
    "    html = driver.page_source\n",
    "    driver.close()\n",
    "    return BeautifulSoup(html, features=\"lxml\")\n",
    "\n",
    "            \n",
    "first = True # These variables were coded in a smarter way when doing the actual analysis\n",
    "scraped = 0  # It used the length of the intermediate_results_par dataset stored on the pc\n",
    "def getAddis(url): \n",
    "    ''' This function is used to extract the html of the additional pages (detail page and amenities page)'''\n",
    "    global first\n",
    "    global scraped\n",
    "    output = pd.DataFrame(columns=[\"details_page\", \"amenities_page\", \"link\"])\n",
    "    try:\n",
    "        dp = getJSpage(url)\n",
    "        output.loc[0] = [dp, getAmenitiesPage(dp), url]\n",
    "    except:\n",
    "        output.loc[0] = [-1, -1, url]\n",
    "    if first: # Ensures that the columns have the correct titles because apparently that's difficult\n",
    "        output.to_csv('Marseille_file_trial.csv', mode='a', header=True, index = False)\n",
    "        first = False\n",
    "    else:\n",
    "        output.to_csv('Marseille_file_trial.csv', mode='a', header=False, index = False) \n",
    "    scraped += 1\n",
    "    print(\"Scraped: {}\".format(scraped))\n",
    "\n",
    "\n",
    "# Extract Javascript enabled information    \n",
    "def getReviews(detailpage):\n",
    "    ''' Returns a list of the featured reviews on the page '''\n",
    "    reviews = detailpage.findAll(class_ = \"_50mnu4\")\n",
    "    output = \"\"\n",
    "    for review in reviews:\n",
    "        output += review.text + \"**-**\" #**-** can be used to split reviews later again\n",
    "    return output\n",
    "\n",
    "\n",
    "def getAmenities(amenitiespage):\n",
    "    amenities = amenitiespage.findAll(class_ = \"_vzrbjl\")\n",
    "    output = \"\"\n",
    "    for amenity in amenities:\n",
    "        output += re.findall('[A-Z][^A-Z]*', amenity.text)[0] + \"**-**\" # **-** will be used to split the string later for the purpose of dummification\n",
    "    return output\n",
    "\n",
    "def getResponseInfo(detailpage):\n",
    "    try:\n",
    "        output = detailpage.find(class_ = \"_jofnfy\").text\n",
    "    except:\n",
    "        output = \"\"\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Clean functions basic data frame extracted using only beautifulsoup\n",
    "'''\n",
    "\n",
    "def cleanFacilities(df): # Treating the facilities as a bag of words to create dummy variables\n",
    "    df.loc[:, \"facilities\"] = df[\"facilities\"].astype(str).str.replace(\"[\",\"\").str.replace(\"]\",\"\")\n",
    "    vectorizer = CountVectorizer(decode_error = \"ignore\") \n",
    "    X = vectorizer.fit_transform(df.facilities)\n",
    "    bag_of_words = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return pd.concat([df.reset_index(drop=True).drop(\"facilities\", axis = 1), bag_of_words], axis=1)\n",
    "\n",
    "def cleanTitle(df):\n",
    "    df.loc[:, \"name\"] = df[\"title\"].str.split(\" null \", n = 0, expand = True)[0].str.replace(\"-\", \"\")\n",
    "    df.loc[:, \"location\"] = df[\"title\"].str.split(\" null \", n = 0, expand = True)[1].str.replace(\"-\", \"\").str.strip()\n",
    "    return df.drop(\"title\", axis = 1)\n",
    "\n",
    "def cleanTopRow(df):\n",
    "    df.loc[:, 'roomtype'] = df[\"toprow\"].str.split(\" in \", n = 0, expand = True)[0] \n",
    "    df.loc[:, 'detailed_location'] = df[\"toprow\"].str.split(\" in \", n = 0, expand = True)[1] \n",
    "    return df.drop(\"toprow\", axis = 1)\n",
    "\n",
    "def cleanRoomInfo(df):\n",
    "    df.loc[:, \"guests\"] = df.loc[:, \"roominfo\"].str.split(\" · \", n = 0, expand = True)[0].str.replace(\" guests\", \"\")\n",
    "    df.loc[:, \"bedrooms\"] = df.loc[:, \"roominfo\"].str.split(\" . \", n = 0, expand = True)[1]\n",
    "    df.loc[:, \"beds\"] = df.loc[:, \"roominfo\"].str.split(\" . \", n = 0, expand = True)[2].str.replace(\" bed\", \"\").str.replace(\"s\", \"\")\n",
    "    df.loc[:, \"bathrooms\"] = df.loc[:, \"roominfo\"].str.split(\" . \", n = 0, expand = True)[3]\n",
    "    df.loc[:, \"guests\"] = pd.to_numeric(df.guests, errors = 'coerce')\n",
    "    df.loc[:, \"beds\"] = pd.to_numeric(df.beds, errors = 'coerce')\n",
    "    df.loc[:, \"bedrooms\"] = pd.to_numeric(df.bedrooms.str.split(\" \", n = 0, expand = True)[0], errors = \"ignore\")\n",
    "    df.loc[:, \"bathrooms\"] = pd.to_numeric(df.bathrooms.str.split(\" \", n = 0, expand = True)[0], errors = \"ignore\")\n",
    "    return df.drop(\"roominfo\", axis = 1)\n",
    "\n",
    "def cleanPrice(df):\n",
    "    df.loc[:, \"pricepernight\"] = df.loc[:, \"price\"].str.split(\"Discounted\", n = 0, expand = True)[0].str.replace(\"$\", \"/\").str.split(\"/\",  n = 0, expand = True)[1]\n",
    "    df.loc[:, 'discountedpricepernight'] = df.loc[:, \"price\"].str.split(\"Discounted\", n = 0, expand = True)[1].str.replace(\"$\", \"/\").str.split(\"/\",  n = 0, expand = True)[1]\n",
    "    df.loc[:, \"price\"] = pd.to_numeric(df.pricepernight.str.replace(\",\",\"\").str.strip())\n",
    "    df.loc[:, \"discountedprice\"] = pd.to_numeric(df.discountedpricepernight.str.replace(\" \", \"\").str.replace(\",\",\"\"), errors = \"coerce\")\n",
    "    return df.drop([\"pricepernight\", \"discountedpricepernight\"], axis = 1)\n",
    "\n",
    "def cleanRating(df):\n",
    "    df.loc[:, \"score\"] = df.loc[:, 'rating'].str.split(\" \", n = 0, expand = True)[1]\n",
    "    df.loc[:, \"score\"] = pd.to_numeric(df.score, errors = \"coerce\")\n",
    "    return df.drop(\"rating\", axis = 1)\n",
    "\n",
    "def cleanReviewNumber(df):\n",
    "    df.loc[:, \"reviewnumber\"] = df.loc[:, 'reviewnumber'].str.split(\" \", n = 0, expand = True)[0]\n",
    "    df.loc[:, \"reviewnumber\"] = pd.to_numeric(df.reviewnumber, errors = \"coerce\")\n",
    "    return df\n",
    "\n",
    "def clean(df):\n",
    "    df = cleanTitle(df)\n",
    "    df = cleanFacilities(df)\n",
    "    df = cleanTopRow(df)\n",
    "    df = cleanRoomInfo(df)\n",
    "    df = cleanPrice(df)\n",
    "    df = cleanRating(df)\n",
    "    df = cleanReviewNumber(df)\n",
    "    # Reorder columns\n",
    "    col1 = df.pop('price')\n",
    "    df = pd.concat([df.reset_index(drop=True), col1], axis=1)\n",
    "    col2 = df.pop('reviewnumber')\n",
    "    df = pd.concat([df.reset_index(drop=True), col2], axis=1) \n",
    "    col3 = df.pop('link')\n",
    "    df = pd.concat([df.reset_index(drop=True), col3], axis=1) \n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "    Clean functions data frame containing the html of the additional pages\n",
    "'''\n",
    "\n",
    "\n",
    "def cleanAmenities(df):\n",
    "    df.loc[:, \"amenities\"] = df.amenities.replace(np.nan, '', regex=True)# fit_transform cannot handle missing values\n",
    "    df.loc[:, \"amenities\"] = df.amenities.str.replace(\" \", \"_\").str.replace(\"-\", \" \").str.replace(\"*\", \"\") #split in two because of a python bug (https://stackoverflow.com/questions/3675144/regex-error-nothing-to-repeat)\n",
    "    vectorizer = CountVectorizer(decode_error = \"ignore\") \n",
    "    X = vectorizer.fit_transform(df.amenities)\n",
    "    bag_of_words = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return pd.concat([df.reset_index(drop=True).drop(\"amenities\", axis = 1), bag_of_words], axis=1)\n",
    "\n",
    "def cleanReviews(df):\n",
    "    df.loc[:, \"reviews\"] = df.reviews.replace(np.nan, '', regex=True)# fit_transform cannot handle missing values\n",
    "    df.loc[:, \"reviews\"] = df.reviews.str.split(\"-\")\n",
    "    return df\n",
    "\n",
    "def getResponseTime(string):\n",
    "    if \"Response time\" in string:\n",
    "        output = string[string.find(\"Response time\") + 15:]\n",
    "    else:\n",
    "        output = \"Unknown\"\n",
    "    return output\n",
    "\n",
    "def getResponseRate(string):\n",
    "    if \"Response rate\" in string:\n",
    "        temp = string[string.find(\"Response rate\") + 15:string.find(\"Response rate\")+20] \n",
    "        output = \"\"\n",
    "        for letter in temp:\n",
    "            if letter in \"0123456789\":\n",
    "                output += letter\n",
    "    else:\n",
    "        output = \"Unknown\"      \n",
    "    return output\n",
    "\n",
    "def getLanguages(string):\n",
    "    if \"Language\" in string:\n",
    "        if \"Response\" in string:\n",
    "            output = string[10:string.find(\"Response\")].strip()\n",
    "        else:\n",
    "            output = string[10:].strip()\n",
    "    else:\n",
    "        output = \"Unknown\"\n",
    "    return output\n",
    "\n",
    "def cleanResponseTime(df):\n",
    "    df.loc[:, \"response_info\"] = df.response_info.replace(np.nan, '', regex=True)\n",
    "    df.loc[:, \"response_time\"] = df.response_info.apply(lambda x: getResponseTime(x))\n",
    "    return df\n",
    "\n",
    "def cleanResponseRate(df):\n",
    "    df.loc[:, \"response_rate\"] = df.response_info.apply(lambda x: getResponseRate(x))\n",
    "    return df\n",
    "\n",
    "def cleanLanguages(df):\n",
    "    df.loc[:, \"languages\"] = df.response_info.apply(lambda x: getLanguages(x))\n",
    "    df.loc[:, \"languages\"] = df.languages.str.split(\",\")\n",
    "    return df\n",
    "\n",
    "def cleanResponseInfo(df):\n",
    "    df = cleanResponseTime(df)\n",
    "    df = cleanResponseRate(df)\n",
    "    df = cleanLanguages(df)\n",
    "    return df.drop(\"response_info\", axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Scraper\n",
    "'''\n",
    "\n",
    "def scraper(urls, sample_size = None, random_state = 1234):\n",
    "    df = scrapeURLs(urls)\n",
    "    df = clean(df)\n",
    "    if sample_size is not None:\n",
    "        df = df.sample(sample_size, random_state = random_state)\n",
    "    Parallel(n_jobs = -1, prefer=\"threads\")(delayed(getAddis)(url) for url in df.link)\n",
    "    df2 = pd.read_csv(\"Marseille_file_trial.csv\")\n",
    "    df = df.merge(df2, on = \"link\")\n",
    "    df.loc[:, 'reviews'] = df.details_page.apply(lambda x: getReviews(BeautifulSoup(x, features = \"lxml\")))\n",
    "    df.loc[:, 'response_info'] = df.details_page.apply(lambda x: getResponseInfo(BeautifulSoup(x, features = \"lxml\")))\n",
    "    df.loc[:, \"amenities\"] = df.amenities_page.apply(lambda x: getAmenities(BeautifulSoup(x, features = \"lxml\")))\n",
    "    df = cleanReviews(df)\n",
    "    df = cleanResponseInfo(df)\n",
    "    #df = cleanAmenities(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "marseille='https://www.airbnb.com/s/Marseille/homes?query=Marseille&source=structured_search_input_header&search_type=autocomplete_click&tab_id=home_tab&checkin=2020-12-24&refinement_paths%5B%5D=%2Fhomes&checkout=2020-12-25&place_id=ChIJM1PaREO_yRIRIAKX_aUZCAQ&locale=en&_set_bev_on_new_domain=1604890805_MzY0NWFlOTg5MjFi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marseille\n",
      "Scraped: 1\n",
      "Scraped: 2\n",
      "Scraped: 3\n",
      "Scraped: 4\n",
      "Scraped: 5\n",
      "Scraped: 6\n",
      "Scraped: 7\n",
      "Scraped: 8\n",
      "Scraped: 9\n",
      "Scraped: 10\n",
      "Scraped: 11\n",
      "Scraped: 12\n",
      "Scraped: 13\n",
      "Scraped: 14\n",
      "Scraped: 15\n",
      "Scraped: 16\n",
      "Scraped: 17\n",
      "Scraped: 18\n",
      "Scraped: 19\n",
      "Scraped: 20\n",
      "Scraped: 21\n",
      "Scraped: 22\n",
      "Scraped: 23\n",
      "Scraped: 24\n",
      "Scraped: 25\n",
      "Scraped: 26\n",
      "Scraped: 27\n",
      "Scraped: 28\n",
      "Scraped: 29\n",
      "Scraped: 30\n",
      "Scraped: 31\n",
      "Scraped: 32\n",
      "Scraped: 33\n",
      "Scraped: 34\n",
      "Scraped: 35\n",
      "Scraped: 36\n",
      "Scraped: 37\n",
      "Scraped: 38\n",
      "Scraped: 39\n",
      "Scraped: 40\n",
      "Scraped: 41\n",
      "Scraped: 42\n",
      "Scraped: 43\n",
      "Scraped: 44\n",
      "Scraped: 45\n",
      "Scraped: 46\n",
      "Scraped: 47\n",
      "Scraped: 48\n",
      "Scraped: 49\n",
      "Scraped: 50\n",
      "Scraped: 51\n",
      "Scraped: 52\n",
      "Scraped: 53\n",
      "Scraped: 54\n",
      "Scraped: 55\n",
      "Scraped: 56\n",
      "Scraped: 57\n",
      "Scraped: 58\n",
      "Scraped: 59\n",
      "Scraped: 60\n",
      "Scraped: 61\n",
      "Scraped: 62\n",
      "Scraped: 63\n",
      "Scraped: 64\n",
      "Scraped: 65\n",
      "Scraped: 66\n",
      "Scraped: 67\n",
      "Scraped: 68\n",
      "Scraped: 69\n",
      "Scraped: 70\n",
      "Scraped: 71\n",
      "Scraped: 72\n",
      "Scraped: 73\n",
      "Scraped: 74\n",
      "Scraped: 75\n",
      "Scraped: 76\n",
      "Scraped: 77\n",
      "Scraped: 78\n",
      "Scraped: 79\n",
      "Scraped: 80\n",
      "Scraped: 81\n",
      "Scraped: 82\n",
      "Scraped: 83\n",
      "Scraped: 84\n",
      "Scraped: 85\n",
      "Scraped: 86\n",
      "Scraped: 87\n",
      "Scraped: 88\n",
      "Scraped: 89\n",
      "Scraped: 90\n",
      "Scraped: 91\n",
      "Scraped: 92\n",
      "Scraped: 93\n",
      "Scraped: 94\n",
      "Scraped: 95\n",
      "Scraped: 96\n",
      "Scraped: 97\n",
      "Scraped: 98\n",
      "Scraped: 99\n",
      "Scraped: 100\n",
      "Scraped: 101\n",
      "Scraped: 102\n",
      "Scraped: 103\n",
      "Scraped: 104\n",
      "Scraped: 105\n",
      "Scraped: 106\n",
      "Scraped: 107\n",
      "Scraped: 108\n",
      "Scraped: 109\n",
      "Scraped: 110\n",
      "Scraped: 111\n",
      "Scraped: 112\n",
      "Scraped: 113\n",
      "Scraped: 114\n",
      "Scraped: 115\n",
      "Scraped: 116\n",
      "Scraped: 117\n",
      "Scraped: 118\n",
      "Scraped: 119\n",
      "Scraped: 120\n",
      "Scraped: 121\n",
      "Scraped: 122\n",
      "Scraped: 123\n",
      "Scraped: 124\n",
      "Scraped: 125\n",
      "Scraped: 126\n",
      "Scraped: 127\n",
      "Scraped: 128\n",
      "Scraped: 129\n",
      "Scraped: 130\n",
      "Scraped: 131\n",
      "Scraped: 132\n",
      "Scraped: 133\n",
      "Scraped: 134\n",
      "Scraped: 135\n",
      "Scraped: 136\n",
      "Scraped: 137\n",
      "Scraped: 138\n",
      "Scraped: 139\n",
      "Scraped: 140\n",
      "Scraped: 141\n",
      "Scraped: 142\n",
      "Scraped: 143\n",
      "Scraped: 144\n",
      "Scraped: 145\n",
      "Scraped: 146\n",
      "Scraped: 147\n",
      "Scraped: 148\n",
      "Scraped: 149\n",
      "Scraped: 150\n",
      "Scraped: 151\n",
      "Scraped: 152\n",
      "Scraped: 153\n",
      "Scraped: 154\n",
      "Scraped: 155\n",
      "Scraped: 156\n",
      "Scraped: 157\n",
      "Scraped: 158\n",
      "Scraped: 159\n",
      "Scraped: 160\n",
      "Scraped: 161\n",
      "Scraped: 162\n",
      "Scraped: 163\n",
      "Scraped: 164\n",
      "Scraped: 165\n",
      "Scraped: 166\n",
      "Scraped: 167\n",
      "Scraped: 168\n",
      "Scraped: 169\n",
      "Scraped: 170\n",
      "Scraped: 171\n",
      "Scraped: 172\n",
      "Scraped: 173\n",
      "Scraped: 174\n",
      "Scraped: 175\n",
      "Scraped: 176\n",
      "Scraped: 177\n",
      "Scraped: 178\n",
      "Scraped: 179\n",
      "Scraped: 180\n",
      "Scraped: 181\n",
      "Scraped: 182\n",
      "Scraped: 183\n",
      "Scraped: 184\n",
      "Scraped: 185\n",
      "Scraped: 186\n",
      "Scraped: 187\n",
      "Scraped: 188\n",
      "Scraped: 189\n",
      "Scraped: 190\n",
      "Scraped: 191\n",
      "Scraped: 192\n",
      "Scraped: 193\n",
      "Scraped: 194\n",
      "Scraped: 195\n",
      "Scraped: 196\n",
      "Scraped: 197\n",
      "Scraped: 198\n",
      "Scraped: 199\n",
      "Scraped: 200\n",
      "Scraped: 201\n",
      "Scraped: 202\n",
      "Scraped: 203\n",
      "Scraped: 204\n",
      "Scraped: 205\n",
      "Scraped: 206\n",
      "Scraped: 207\n",
      "Scraped: 208\n",
      "Scraped: 209\n",
      "Scraped: 210\n",
      "Scraped: 211\n",
      "Scraped: 212\n",
      "Scraped: 213\n",
      "Scraped: 214\n",
      "Scraped: 215\n",
      "Scraped: 216\n",
      "Scraped: 217\n",
      "Scraped: 218\n",
      "Scraped: 219\n",
      "Scraped: 220\n",
      "Scraped: 221\n",
      "Scraped: 222\n",
      "Scraped: 223\n",
      "Scraped: 224\n",
      "Scraped: 225\n",
      "Scraped: 226\n",
      "Scraped: 227\n",
      "Scraped: 228\n",
      "Scraped: 229\n",
      "Scraped: 230\n",
      "Scraped: 231\n",
      "Scraped: 232\n",
      "Scraped: 233\n",
      "Scraped: 234\n",
      "Scraped: 235\n",
      "Scraped: 236\n",
      "Scraped: 237\n",
      "Scraped: 238\n",
      "Scraped: 239\n",
      "Scraped: 240\n",
      "Scraped: 241\n",
      "Scraped: 242\n",
      "Scraped: 243\n",
      "Scraped: 244\n",
      "Scraped: 245\n",
      "Scraped: 246\n",
      "Scraped: 247\n",
      "Scraped: 248\n",
      "Scraped: 249\n",
      "Scraped: 250\n",
      "Scraped: 251\n",
      "Scraped: 252\n",
      "Scraped: 253\n",
      "Scraped: 254\n",
      "Scraped: 255\n",
      "Scraped: 256\n",
      "Scraped: 257\n",
      "Scraped: 258\n",
      "Scraped: 259\n",
      "Scraped: 260\n",
      "Scraped: 261\n",
      "Scraped: 262\n",
      "Scraped: 263\n",
      "Scraped: 264\n",
      "Scraped: 265\n",
      "Scraped: 266\n",
      "Scraped: 267\n",
      "Scraped: 268\n",
      "Scraped: 269\n",
      "Scraped: 270\n",
      "Scraped: 271\n",
      "Scraped: 272\n",
      "Scraped: 273\n",
      "Scraped: 274\n",
      "Scraped: 275\n",
      "Scraped: 276\n",
      "Scraped: 277\n",
      "Scraped: 278\n",
      "Scraped: 279\n",
      "Scraped: 280\n",
      "Scraped: 281\n",
      "Scraped: 282\n",
      "Scraped: 283\n",
      "Scraped: 284\n",
      "Scraped: 285\n",
      "Scraped: 286\n",
      "Scraped: 287\n",
      "Scraped: 288\n",
      "Scraped: 289\n",
      "Scraped: 290\n",
      "Scraped: 291\n",
      "Scraped: 292\n",
      "Scraped: 293\n",
      "Scraped: 294\n",
      "Scraped: 295\n",
      "Scraped: 296\n",
      "Scraped: 297\n",
      "Scraped: 298\n",
      "Scraped: 299\n",
      "Scraped: 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls2 = [[\"Marseille\", marseille]]\n",
    "df = scraper(urls2,)\n",
    "\n",
    "df.to_csv(\"Marseille_file_trial\", index=False)\n",
    "del marseille,first, scraped, urls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
