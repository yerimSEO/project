{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd001f139f06680331c19aeaba26688dba45234c5fc4176058c2178ac70640604d9",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Yelp Project\n",
    "\n",
    "**Group 1**:\n",
    "- SEO Yerim\n",
    "- TIRUMALE LAKSHMANA RAO Kiran\n",
    "- ZHENG Bin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Using the 6 datasets provided by Yelp, we as a team worked on building a prediction model to forecast which businesses will start doing delivery/takeout after the first lockdown in the North American region.\n",
    "\n",
    "In order to help businesses sustain during this Covid-19 pandemic, it's important to undestand which key factors contributed to businesses sustaining themselves post the first wave of covid 19."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Project Pipeline:\n",
    "\n",
    "- **Reading in the Data** - Understanding all 6 datasets provided by Covid\n",
    "- **Dropping Unnecessary Columns** - Dropping features that wouldn't contribute to the model's performance, and also dropping complex features.\n",
    "- **Joining all 6 datasets** - forming a basetable - Joining all datasets, after transforming them, in order to obtain 1 observation per business\n",
    "- **Preprocessing the basetable** - Cleaning the basetable, handling missing values, pre-processing categorical features and text features for modeling\n",
    "- **Modeling** - Modeling using 4 different algorithms using hyper parameter tuning, also finding the important features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Reading in the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "source": [
    "### Reading in the Yelp Data\n",
    "https://www.yelp.com/dataset/documentation/main\n",
    "\n",
    "Undertanding the all the datasets provided by covid, and doing performing an initial analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['data_exports', 'OneDrive_1_12-04-2021.zip', 'parsed_business_sample.json', 'parsed_checkin_sample.json', 'parsed_covid_sample.json', 'parsed_review_sample.json', 'parsed_tip_sample.json', 'parsed_user_sample.json']\n"
     ]
    }
   ],
   "source": [
    "# Listing all the datasets available to us\n",
    "import os\n",
    "print(os.listdir(\"./data4\"))"
   ]
  },
  {
   "source": [
    "### Business Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['address', 'attributes.AcceptsInsurance', 'attributes.AgesAllowed', 'attributes.Alcohol', 'attributes.Ambience', 'attributes.BYOB', 'attributes.BYOBCorkage', 'attributes.BestNights', 'attributes.BikeParking', 'attributes.BusinessAcceptsBitcoin', 'attributes.BusinessAcceptsCreditCards', 'attributes.BusinessParking', 'attributes.ByAppointmentOnly', 'attributes.Caters', 'attributes.CoatCheck', 'attributes.Corkage', 'attributes.DietaryRestrictions', 'attributes.DogsAllowed', 'attributes.DriveThru', 'attributes.GoodForDancing', 'attributes.GoodForKids', 'attributes.GoodForMeal', 'attributes.HairSpecializesIn', 'attributes.HappyHour', 'attributes.HasTV', 'attributes.Music', 'attributes.NoiseLevel', 'attributes.Open24Hours', 'attributes.OutdoorSeating', 'attributes.RestaurantsAttire', 'attributes.RestaurantsCounterService', 'attributes.RestaurantsDelivery', 'attributes.RestaurantsGoodForGroups', 'attributes.RestaurantsPriceRange2', 'attributes.RestaurantsReservations', 'attributes.RestaurantsTableService', 'attributes.RestaurantsTakeOut', 'attributes.Smoking', 'attributes.WheelchairAccessible', 'attributes.WiFi', 'business_id', 'categories', 'city', 'hours.Friday', 'hours.Monday', 'hours.Saturday', 'hours.Sunday', 'hours.Thursday', 'hours.Tuesday', 'hours.Wednesday', 'is_open', 'latitude', 'longitude', 'name', 'postal_code', 'review_count', 'stars', 'state']\n",
      "(19018, 58)\n"
     ]
    }
   ],
   "source": [
    "# Business Data - Contains business data including location data, attributes, and categories.\n",
    "business = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_business_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(business.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((business.count(), len(business.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace . in the column name into _  and remove spaces in columns name\n",
    "\n",
    "for name in business.schema.names:\n",
    "  business = business.withColumnRenamed(name, name.replace('.', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- address: string (nullable = true)\n |-- attributes_AcceptsInsurance: string (nullable = true)\n |-- attributes_AgesAllowed: string (nullable = true)\n |-- attributes_Alcohol: string (nullable = true)\n |-- attributes_Ambience: string (nullable = true)\n |-- attributes_BYOB: string (nullable = true)\n |-- attributes_BYOBCorkage: string (nullable = true)\n |-- attributes_BestNights: string (nullable = true)\n |-- attributes_BikeParking: string (nullable = true)\n |-- attributes_BusinessAcceptsBitcoin: string (nullable = true)\n |-- attributes_BusinessAcceptsCreditCards: string (nullable = true)\n |-- attributes_BusinessParking: string (nullable = true)\n |-- attributes_ByAppointmentOnly: string (nullable = true)\n |-- attributes_Caters: string (nullable = true)\n |-- attributes_CoatCheck: string (nullable = true)\n |-- attributes_Corkage: string (nullable = true)\n |-- attributes_DietaryRestrictions: string (nullable = true)\n |-- attributes_DogsAllowed: string (nullable = true)\n |-- attributes_DriveThru: string (nullable = true)\n |-- attributes_GoodForDancing: string (nullable = true)\n |-- attributes_GoodForKids: string (nullable = true)\n |-- attributes_GoodForMeal: string (nullable = true)\n |-- attributes_HairSpecializesIn: string (nullable = true)\n |-- attributes_HappyHour: string (nullable = true)\n |-- attributes_HasTV: string (nullable = true)\n |-- attributes_Music: string (nullable = true)\n |-- attributes_NoiseLevel: string (nullable = true)\n |-- attributes_Open24Hours: string (nullable = true)\n |-- attributes_OutdoorSeating: string (nullable = true)\n |-- attributes_RestaurantsAttire: string (nullable = true)\n |-- attributes_RestaurantsCounterService: string (nullable = true)\n |-- attributes_RestaurantsDelivery: string (nullable = true)\n |-- attributes_RestaurantsGoodForGroups: string (nullable = true)\n |-- attributes_RestaurantsPriceRange2: string (nullable = true)\n |-- attributes_RestaurantsReservations: string (nullable = true)\n |-- attributes_RestaurantsTableService: string (nullable = true)\n |-- attributes_RestaurantsTakeOut: string (nullable = true)\n |-- attributes_Smoking: string (nullable = true)\n |-- attributes_WheelchairAccessible: string (nullable = true)\n |-- attributes_WiFi: string (nullable = true)\n |-- business_id: string (nullable = true)\n |-- categories: string (nullable = true)\n |-- city: string (nullable = true)\n |-- hours_Friday: string (nullable = true)\n |-- hours_Monday: string (nullable = true)\n |-- hours_Saturday: string (nullable = true)\n |-- hours_Sunday: string (nullable = true)\n |-- hours_Thursday: string (nullable = true)\n |-- hours_Tuesday: string (nullable = true)\n |-- hours_Wednesday: string (nullable = true)\n |-- is_open: long (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- name: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- stars: double (nullable = true)\n |-- state: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Checking the schema as well as confirming the renaming of all columns\n",
    "business.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "business.select('business_id').distinct().count()"
   ]
  },
  {
   "source": [
    "### Checkin Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['business_id', 'date']\n",
      "(1990914, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checkin Data - Checkins on a business.\n",
    "checkin = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_checkin_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(checkin.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((checkin.count(), len(checkin.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+--------------------+\n|business_id           |date                |\n+----------------------+--------------------+\n|hihud--QRriCYZw1zZvW4g|2013-03-17 00:41:53 |\n|hihud--QRriCYZw1zZvW4g| 2013-03-17 02:41:06|\n|hihud--QRriCYZw1zZvW4g| 2013-03-18 06:26:54|\n|hihud--QRriCYZw1zZvW4g| 2013-03-19 00:13:05|\n|hihud--QRriCYZw1zZvW4g| 2013-03-23 04:27:56|\n|hihud--QRriCYZw1zZvW4g| 2013-03-30 01:18:59|\n|hihud--QRriCYZw1zZvW4g| 2013-04-02 04:57:07|\n|hihud--QRriCYZw1zZvW4g| 2013-04-04 02:15:51|\n|hihud--QRriCYZw1zZvW4g| 2013-04-04 08:44:58|\n|hihud--QRriCYZw1zZvW4g| 2013-04-07 06:25:42|\n+----------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "checkin.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16244"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "checkin.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "checkin=checkin.withColumn('date', regexp_replace('date', '&#39', ''))\n",
    "\n",
    "checkin=checkin.select(\"business_id\",to_timestamp(checkin.date, 'yyyy-MM-dd HH:mm:ss').alias('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+-------------------+\n|business_id           |date               |\n+----------------------+-------------------+\n|hihud--QRriCYZw1zZvW4g|2013-03-17 00:41:53|\n|hihud--QRriCYZw1zZvW4g|2013-03-17 02:41:06|\n|hihud--QRriCYZw1zZvW4g|2013-03-18 06:26:54|\n|hihud--QRriCYZw1zZvW4g|2013-03-19 00:13:05|\n|hihud--QRriCYZw1zZvW4g|2013-03-23 04:27:56|\n|hihud--QRriCYZw1zZvW4g|2013-03-30 01:18:59|\n|hihud--QRriCYZw1zZvW4g|2013-04-02 04:57:07|\n|hihud--QRriCYZw1zZvW4g|2013-04-04 02:15:51|\n|hihud--QRriCYZw1zZvW4g|2013-04-04 08:44:58|\n|hihud--QRriCYZw1zZvW4g|2013-04-07 06:25:42|\n+----------------------+-------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "checkin.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Review Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text', 'useful', 'user_id']\n",
      "(500000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Review Data - Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n",
    "review = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_review_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(review.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((review.count(), len(review.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- business_id: string (nullable = true)\n |-- cool: long (nullable = true)\n |-- date: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- review_id: string (nullable = true)\n |-- stars: long (nullable = true)\n |-- text: string (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "review.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "305084"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Checking the unique values of user_id\n",
    "review.select('user_id').distinct().count()"
   ]
  },
  {
   "source": [
    "### Tip Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['business_id', 'compliment_count', 'date', 'text', 'user_id']\n",
      "(124161, 5)\n"
     ]
    }
   ],
   "source": [
    "# Tip Data - Tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions.\n",
    "tip = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_tip_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(tip.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((tip.count(), len(tip.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- business_id: string (nullable = true)\n |-- compliment_count: long (nullable = true)\n |-- date: string (nullable = true)\n |-- text: string (nullable = true)\n |-- user_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "tip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12578"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "tip.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "68774"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "tip.select('user_id').distinct().count()"
   ]
  },
  {
   "source": [
    "### User Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['average_stars', 'compliment_cool', 'compliment_cute', 'compliment_funny', 'compliment_hot', 'compliment_list', 'compliment_more', 'compliment_note', 'compliment_photos', 'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool', 'elite', 'fans', 'friends', 'funny', 'name', 'review_count', 'useful', 'user_id', 'yelping_since']\n",
      "(305084, 22)\n"
     ]
    }
   ],
   "source": [
    "# User Data - User data including the user's friend mapping and all the metadata associated with the user.\n",
    "user = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_user_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(user.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((user.count(), len(user.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- average_stars: double (nullable = true)\n |-- compliment_cool: long (nullable = true)\n |-- compliment_cute: long (nullable = true)\n |-- compliment_funny: long (nullable = true)\n |-- compliment_hot: long (nullable = true)\n |-- compliment_list: long (nullable = true)\n |-- compliment_more: long (nullable = true)\n |-- compliment_note: long (nullable = true)\n |-- compliment_photos: long (nullable = true)\n |-- compliment_plain: long (nullable = true)\n |-- compliment_profile: long (nullable = true)\n |-- compliment_writer: long (nullable = true)\n |-- cool: long (nullable = true)\n |-- elite: string (nullable = true)\n |-- fans: long (nullable = true)\n |-- friends: string (nullable = true)\n |-- funny: long (nullable = true)\n |-- name: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)\n |-- yelping_since: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "305084"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Checking the unique values of user_id\n",
    "user.select('user_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Covid Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Call To Action enabled', 'Covid Banner', 'Grubhub enabled', 'Request a Quote Enabled', 'Temporary Closed Until', 'Virtual Services Offered', 'business_id', 'delivery or takeout', 'highlights']\n(19053, 9)\n"
     ]
    }
   ],
   "source": [
    "# Reading in the Covid Features Data\n",
    "covid = spark.read.json(\"/FileStore/tables/Yelp_Data_Group/parsed_covid_sample.json\")\n",
    "\n",
    "# Viewing the data\n",
    "print(covid.columns)\n",
    "\n",
    "# Re-checking the shape of the review user dataset loaded from json\n",
    "print((covid.count(), len(covid.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- Call To Action enabled: string (nullable = true)\n |-- Covid Banner: string (nullable = true)\n |-- Grubhub enabled: string (nullable = true)\n |-- Request a Quote Enabled: string (nullable = true)\n |-- Temporary Closed Until: string (nullable = true)\n |-- Virtual Services Offered: string (nullable = true)\n |-- business_id: string (nullable = true)\n |-- delivery or takeout: string (nullable = true)\n |-- highlights: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "covid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "covid.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "covid = covid.dropDuplicates([\"business_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+\n",
      "|delivery or takeout|\n",
      "+-------------------+\n",
      "|FALSE              |\n",
      "|TRUE               |\n",
      "+-------------------+\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|count(CASE WHEN isnan(delivery or takeout) THEN true END)|\n",
      "+---------------------------------------------------------+\n",
      "|                                                        0|\n",
      "+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check distinct values of the column\n",
    "covid.select('delivery or takeout').distinct().show(truncate = False)\n",
    "\n",
    "### Get count of both null and missing values for the column 'delivery or takeout'\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "covid.select([count(when(isnan('delivery or takeout'),True))]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-----+\n|delivery or takeout|count|\n+-------------------+-----+\n|              FALSE|12794|\n|               TRUE| 6224|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Checking the count of both false and trues in the column 'delivery or takeout'\n",
    "covid.select('delivery or takeout').groupBy('delivery or takeout').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n|sum(count)|\n+----------+\n|      null|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate rows in the covid dataset\n",
    "import pyspark.sql.functions as f\n",
    "covid.groupBy(covid.columns)\\\n",
    "    .count()\\\n",
    "    .where(f.col('count') > 1)\\\n",
    "    .select(f.sum('count'))\\\n",
    "    .show()"
   ]
  },
  {
   "source": [
    "# Dropping Unnecessary Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before merging all above datasets into a single basetable, let us drop columns that we will not require for our machine learning algorithms, or in other words, columns that we feel will not contribute in improving the prediction capabilities of the algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns of each dataset to be dropped\n",
    "\n",
    "business_drop = ['address',  'name',  'attributes_Ambience', 'attributes_BYOBCorkage', 'attributes_BestNights', 'attributes_BusinessParking', 'attributes_DietaryRestrictions', 'attributes_GoodForMeal', 'attributes_HairSpecializesIn', 'attributes_Music', 'attributes_NoiseLevel', 'attributes_RestaurantsAttire', 'attributes_Smoking', 'attributes_WiFi', 'hours_Monday', 'hours_Tuesday', 'hours_Wednesday', 'hours_Thursday', 'hours_Friday', 'hours_Saturday', 'hours_Sunday', 'attributes_RestaurantsPriceRange2', 'latitude', 'longitude']\n",
    "# 'city', 'categories', 'postal_code', 'state', 'attributes_AgesAllowed', 'attributes_Alcohol', 'latitude', 'longitude'\n",
    "\n",
    "review_drop = ['date', 'review_id', 'text']\n",
    "\n",
    "tip_drop = ['text', 'date']\n",
    "\n",
    "user_drop = ['elite', 'friends', 'name']\n",
    "\n",
    "covid_drop = ['highlights', 'Temporary Closed Until']\n",
    "# 'Virtual Services Offered', 'Covid Banner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns from the dataframes\n",
    "business = business.drop(*business_drop)\n",
    "review = review.drop(*review_drop)\n",
    "tip = tip.drop(*tip_drop)\n",
    "user = user.drop(*user_drop)\n",
    "covid = covid.drop(*covid_drop)"
   ]
  },
  {
   "source": [
    "# Joining the tables to form the `basetable`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Joining the `Review` and `User` Dataframes into `review_user`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First renaming the `cool` and `funny`columns from the Review dataframe\n",
    "review = review.withColumnRenamed(\"cool\", \"review_cool\")\n",
    "review = review.withColumnRenamed(\"funny\", \"review_funny\")\n",
    "review = review.withColumnRenamed(\"useful\", \"review_useful\")"
   ]
  },
  {
   "source": [
    "Step 1: Performing an inner joining between the 2 dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing an inner join of the review and user dataframes\n",
    "review_user = review.join(user,review.user_id ==  user.user_id,\"inner\").drop(user.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(500000, 24)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((review_user.count(), len(review_user.columns)))"
   ]
  },
  {
   "source": [
    "Step 2: grouping by and aggregating the values by business_id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the user_id column\n",
    "review_user = review_user.drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_user = review_user.groupBy(\"business_id\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 22)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((review_user.count(), len(review_user.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "review_user.select('business_id').distinct().count()"
   ]
  },
  {
   "source": [
    "### Joining the `review_user` dataframe with `business` into `yelp_datamart`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing an inner join of the review_user and business_flatten dataframes\n",
    "yelp_datamart = business.join(review_user, business.business_id ==  review_user.business_id,\"inner\").drop(review_user.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 55)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((yelp_datamart.count(), len(yelp_datamart.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19018"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Checking the unique values of business_id\n",
    "yelp_datamart.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Joining the `tip` dataframe to the `yelp_datamart`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 1: Processing the `tip` data, groupby business_id, and aggregating by sum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip = tip.select(\"business_id\", \"compliment_count\").groupBy(\"business_id\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12578"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tip.select(\"business_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column back to compliment_count\n",
    "tip = tip.withColumnRenamed(\"sum(compliment_count)\", \"compliment_count\")"
   ]
  },
  {
   "source": [
    "Step 2: Performing a left join\n",
    "\n",
    "Given that we have only 12578 business_id with compliment counts, we shall perform a left join"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing an inner join of the review_user and business_flatten dataframes\n",
    "yelp_datamart = yelp_datamart.join(tip, yelp_datamart.business_id ==  tip.business_id,\"left\").drop(tip.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 56)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((yelp_datamart.count(), len(yelp_datamart.columns)))"
   ]
  },
  {
   "source": [
    "### Joining the `checkin` dataframe to the `yelp_datamart`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Step 1: Processing the `checkin` data, groupby `business_id`, and aggregating by `count`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin = checkin.groupBy(\"business_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column back to compliment_count\n",
    "checkin = checkin.withColumnRenamed(\"count\", \"checkin_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16244"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# Checking number of unique business ids\n",
    "checkin.select(\"business_id\").distinct().count()"
   ]
  },
  {
   "source": [
    "Step 2: Performing a left join\n",
    "\n",
    "Given that we have only 16244 business_id with checkin counts, we shall perform a left join"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing an inner join of the review_user and business_flatten dataframes\n",
    "yelp_datamart = yelp_datamart.join(checkin, yelp_datamart.business_id ==  checkin.business_id,\"left\").drop(checkin.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 57)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((yelp_datamart.count(), len(yelp_datamart.columns)))"
   ]
  },
  {
   "source": [
    "### Joining the `covid` dataframe to the `yelp_datamart`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing an inner join of the yelp_datamart and covid dataframes\n",
    "yelp_datamart = yelp_datamart.join(covid, yelp_datamart.business_id ==  covid.business_id,\"inner\").drop(covid.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 63)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the joined dataframe\n",
    "print((yelp_datamart.count(), len(yelp_datamart.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-----+\n|delivery or takeout|count|\n+-------------------+-----+\n|              FALSE|12794|\n|               TRUE| 6224|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "yelp_datamart.select('delivery or takeout').groupBy('delivery or takeout').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = yelp_datamart"
   ]
  },
  {
   "source": [
    "# Pre-Processing the Basetable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Checking for missing values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 63)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the basetable\n",
    "print((basetable.count(), len(basetable.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 0\n",
       "attributes_AcceptsInsurance  18318\n",
       "attributes_AgesAllowed       19006\n",
       "attributes_Alcohol           14163\n",
       "attributes_BYOB              18977\n",
       "attributes_BikeParking       10541\n",
       "...                            ...\n",
       "Covid Banner                     0\n",
       "Grubhub enabled                  0\n",
       "Request a Quote Enabled          0\n",
       "Virtual Services Offered         0\n",
       "delivery or takeout              0\n",
       "\n",
       "[63 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>attributes_AcceptsInsurance</th>\n      <td>18318</td>\n    </tr>\n    <tr>\n      <th>attributes_AgesAllowed</th>\n      <td>19006</td>\n    </tr>\n    <tr>\n      <th>attributes_Alcohol</th>\n      <td>14163</td>\n    </tr>\n    <tr>\n      <th>attributes_BYOB</th>\n      <td>18977</td>\n    </tr>\n    <tr>\n      <th>attributes_BikeParking</th>\n      <td>10541</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Covid Banner</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Grubhub enabled</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Request a Quote Enabled</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Virtual Services Offered</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>delivery or takeout</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>63 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# Checking for missing values in the Basetable\n",
    "from pyspark.sql.functions import when, count, col\n",
    "basetable.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "           basetable.columns]).toPandas().T"
   ]
  },
  {
   "source": [
    "### Dropping columns with over 80% missing values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "80% missing values of a basetable of 19018 rows is 15214 missing value per column. \n",
    "\n",
    "Hence, if any column has more than 15200 missing values, we will are dropping that column!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with over 15200 missing values:\n",
    "missing_cols = ['attributes_AcceptsInsurance', 'attributes_AgesAllowed', 'attributes_BYOB', 'attributes_BusinessAcceptsBitcoin', 'attributes_Caters', 'attributes_CoatCheck', 'attributes_Corkage', 'attributes_DogsAllowed', 'attributes_DriveThru', 'attributes_GoodForDancing', 'attributes_HappyHour', 'attributes_Open24Hours', 'attributes_RestaurantsCounterService', 'attributes_RestaurantsTableService', 'attributes_WheelchairAccessible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all above columns that have over 80% missing values\n",
    "basetable = basetable.drop(*missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all missing values with -1\n",
    "basetable = basetable.na.fill(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 48)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the basetable\n",
    "print((basetable.count(), len(basetable.columns)))"
   ]
  },
  {
   "source": [
    "###  Converting boolean columns to integer values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have a list of columns that should only contain boolean values (True (1) and False(0)). Let us pre-process first these columns, by converting all Trues to 1 and all Falses to 0, and any other values to -1 indicating a missing value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of boolean columns\n",
    "cols=['attributes_BikeParking','attributes_BusinessAcceptsCreditCards','attributes_ByAppointmentOnly','attributes_GoodForKids','attributes_HasTV','attributes_OutdoorSeating','attributes_RestaurantsDelivery','attributes_RestaurantsGoodForGroups','attributes_RestaurantsReservations','attributes_RestaurantsTakeOut', 'Call To Action enabled','Grubhub enabled','Request a Quote Enabled','delivery or takeout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in cols:\n",
    "  basetable = basetable.withColumn(col_name, when(basetable[col_name] == \"True\", 1)\n",
    "                                 .when(basetable[col_name] == \"TRUE\", 1)\n",
    "                                 .when(basetable[col_name] == \"False\", 0)\n",
    "                                 .when(basetable[col_name] == \"FALSE\", 0)\n",
    "                                 .otherwise(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+-----+\n|delivery or takeout|count|\n+-------------------+-----+\n|                  1| 6224|\n|                  0|12794|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Confirming the above change, by checking the delivery or takeout column\n",
    "basetable.select('delivery or takeout').groupBy('delivery or takeout').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           0\n",
       "attributes_Alcohol                     14163\n",
       "attributes_BikeParking                     0\n",
       "attributes_BusinessAcceptsCreditCards      0\n",
       "attributes_ByAppointmentOnly               0\n",
       "attributes_GoodForKids                     0\n",
       "attributes_HasTV                           0\n",
       "attributes_OutdoorSeating                  0\n",
       "attributes_RestaurantsDelivery             0\n",
       "attributes_RestaurantsGoodForGroups        0\n",
       "attributes_RestaurantsReservations         0\n",
       "attributes_RestaurantsTakeOut              0\n",
       "business_id                                0\n",
       "categories                                39\n",
       "city                                       0\n",
       "is_open                                    0\n",
       "postal_code                                0\n",
       "review_count                               0\n",
       "stars                                      0\n",
       "state                                      0\n",
       "avg(review_cool)                           0\n",
       "avg(review_funny)                          0\n",
       "avg(stars)                                 0\n",
       "avg(review_useful)                         0\n",
       "avg(average_stars)                         0\n",
       "avg(compliment_cool)                       0\n",
       "avg(compliment_cute)                       0\n",
       "avg(compliment_funny)                      0\n",
       "avg(compliment_hot)                        0\n",
       "avg(compliment_list)                       0\n",
       "avg(compliment_more)                       0\n",
       "avg(compliment_note)                       0\n",
       "avg(compliment_photos)                     0\n",
       "avg(compliment_plain)                      0\n",
       "avg(compliment_profile)                    0\n",
       "avg(compliment_writer)                     0\n",
       "avg(cool)                                  0\n",
       "avg(fans)                                  0\n",
       "avg(funny)                                 0\n",
       "avg(review_count)                          0\n",
       "avg(useful)                                0\n",
       "compliment_count                           0\n",
       "checkin_count                              0\n",
       "Call To Action enabled                     0\n",
       "Covid Banner                               0\n",
       "Grubhub enabled                            0\n",
       "Request a Quote Enabled                    0\n",
       "Virtual Services Offered                   0\n",
       "delivery or takeout                        0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>attributes_Alcohol</th>\n      <td>14163</td>\n    </tr>\n    <tr>\n      <th>attributes_BikeParking</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_BusinessAcceptsCreditCards</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_ByAppointmentOnly</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_GoodForKids</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_HasTV</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_OutdoorSeating</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_RestaurantsDelivery</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_RestaurantsGoodForGroups</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_RestaurantsReservations</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>attributes_RestaurantsTakeOut</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>business_id</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>categories</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>city</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>is_open</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>postal_code</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>review_count</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>stars</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>state</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(review_cool)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(review_funny)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(stars)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(review_useful)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(average_stars)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_cool)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_cute)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_funny)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_hot)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_list)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_more)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_note)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_photos)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_plain)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_profile)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(compliment_writer)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(cool)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(fans)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(funny)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(review_count)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>avg(useful)</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>compliment_count</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>checkin_count</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Call To Action enabled</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Covid Banner</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Grubhub enabled</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Request a Quote Enabled</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Virtual Services Offered</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>delivery or takeout</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# Checking for missing values in the cleaned and processed Basetable\n",
    "from pyspark.sql.functions import when, count, col\n",
    "basetable.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "           basetable.columns]).toPandas().T"
   ]
  },
  {
   "source": [
    "#### Handling the `Virtual Services Offered` column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------+\n|Virtual Services Offered       |\n+-------------------------------+\n|FALSE                          |\n|offers_virtual_tasting_sessions|\n|offers_virtual_experiences     |\n|offers_virtual_tours           |\n|offers_virtual_performances    |\n|offers_virtual_classes         |\n|offers_virtual_consultations   |\n+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Checking the unique values of the virtual services offered column\n",
    "basetable.select(\"Virtual Services Offered\").distinct().show(truncate=False)"
   ]
  },
  {
   "source": [
    "Given that we have only False, and the rest of the values indicate a True, meaning virtual services were offered.\n",
    "Let's replace all values other than False with True = 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = basetable.withColumn(\"Virtual Services Offered\", when(basetable[\"Virtual Services Offered\"] == \"FALSE\", 0)\n",
    "                                 .when(basetable[\"Virtual Services Offered\"] == \"False\", 0)\n",
    "                                 .otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------+\n|Virtual Services Offered|\n+------------------------+\n|1                       |\n|0                       |\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Now confirming the above changes made to the virtual services offered column\n",
    "basetable.select(\"Virtual Services Offered\").distinct().show(truncate=False)"
   ]
  },
  {
   "source": [
    "### Converting the Target column to Double format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Converting the target column `delivery or takeout` to double format and renaming it to `label`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = basetable.withColumn(\"delivery or takeout\", basetable[\"delivery or takeout\"].cast(\"double\"))\\\n",
    "                     .withColumnRenamed(\"delivery or takeout\",\"label\")"
   ]
  },
  {
   "source": [
    "### Processing the categorical columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**STEP 1: The following categorical columns need to be processed using `StringIndexer`**\n",
    "\n",
    "- `attributes_Alcohol`\n",
    "- `city`\n",
    "- `postal_code`\n",
    "- `state`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        0\n",
       "attributes_Alcohol  14163\n",
       "city                    0\n",
       "postal_code             0\n",
       "state                   0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>attributes_Alcohol</th>\n      <td>14163</td>\n    </tr>\n    <tr>\n      <th>city</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>postal_code</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>state</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "# Checking for missing values for above 5 columns\n",
    "\n",
    "cat_cols = ['attributes_Alcohol', 'city', 'postal_code', 'state']\n",
    "\n",
    "from pyspark.sql.functions import when, count, col\n",
    "basetable.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "           cat_cols]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing vavlues for the above column with -1\n",
    "basetable = basetable.fillna( {'attributes_Alcohol':-1 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# attributes_Alcohol\n",
    "alcoholIndxr = StringIndexer().setInputCol(\"attributes_Alcohol\").setOutputCol(\"Alcohol_Ind\")\n",
    "\n",
    "# city\n",
    "cityIndxr = StringIndexer().setInputCol(\"city\").setOutputCol(\"city_Ind\")\n",
    "\n",
    "# postal_code\n",
    "postal_codeIndxr = StringIndexer().setInputCol(\"postal_code\").setOutputCol(\"postal_code_Ind\")\n",
    "\n",
    "# state\n",
    "stateIndxr = StringIndexer().setInputCol(\"state\").setOutputCol(\"state_Ind\")\n",
    "\n",
    "\n",
    "pipe_catv = Pipeline(stages=[alcoholIndxr, cityIndxr, postal_codeIndxr, stateIndxr])\n",
    "basetable = pipe_catv.fit(basetable).transform(basetable)\n",
    "basetable = basetable.drop(\"attributes_Alcohol\", \"city\", \"postal_code\", \"state\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------+---------------+---------+\n|Alcohol_Ind|city_Ind|postal_code_Ind|state_Ind|\n+-----------+--------+---------------+---------+\n|        0.0|     0.0|           74.0|      1.0|\n|        0.0|    14.0|           28.0|      0.0|\n|        0.0|     1.0|         3656.0|      2.0|\n+-----------+--------+---------------+---------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Seeing the above changes\n",
    "basetable.select('Alcohol_Ind', 'city_Ind', 'postal_code_Ind', 'state_Ind').show(3)"
   ]
  },
  {
   "source": [
    "**STEP 2: Let us now onehot encode all boolean columns we cleaned earlier.**\n",
    "\n",
    "Given that we have a lot of columns in this boolean method, let's apply the onehot encoding method using a loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols=['attributes_BikeParking','attributes_BusinessAcceptsCreditCards','attributes_ByAppointmentOnly','attributes_GoodForKids','attributes_HasTV','attributes_OutdoorSeating','attributes_RestaurantsDelivery','attributes_RestaurantsGoodForGroups','attributes_RestaurantsReservations','attributes_RestaurantsTakeOut','is_open', 'Call To Action enabled','Grubhub enabled','Request a Quote Enabled','Virtual Services Offered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first applying the stringindexer, then onehotencoding in a loop for all columns from bool_cols\n",
    "for my_col in bool_cols:\n",
    "    my_col_ind = my_col + \"Ind\"\n",
    "    my_col_dum = my_col + \"_dum\"\n",
    "    model = StringIndexer().setInputCol(my_col).setOutputCol(my_col_ind)\n",
    "    ohe = OneHotEncoder(inputCols=[my_col_ind],outputCols=[my_col_dum])\n",
    "    pipe = Pipeline(stages=[model, ohe])\n",
    "    basetable = pipe.fit(basetable).transform(basetable)\n",
    "    basetable = basetable.drop(my_col, my_col_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------+-----------------------------------------+--------------------------------+\n|attributes_BikeParking_dum|attributes_BusinessAcceptsCreditCards_dum|attributes_ByAppointmentOnly_dum|\n+--------------------------+-----------------------------------------+--------------------------------+\n|             (2,[0],[1.0])|                            (2,[1],[1.0])|                   (2,[0],[1.0])|\n|             (2,[0],[1.0])|                            (2,[0],[1.0])|                   (2,[0],[1.0])|\n|             (2,[0],[1.0])|                            (2,[1],[1.0])|                   (2,[0],[1.0])|\n+--------------------------+-----------------------------------------+--------------------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Checking the above changes on the first 3 columns updated\n",
    "basetable.select('attributes_BikeParking_dum','attributes_BusinessAcceptsCreditCards_dum','attributes_ByAppointmentOnly_dum').show(3)"
   ]
  },
  {
   "source": [
    "### Processing the text column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have only 2 text columns \n",
    "- `categories`\n",
    "- `Covid Banner`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               0\n",
       "categories    39\n",
       "Covid Banner   0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>categories</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>Covid Banner</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "# Checking for missing values of above 2 columns\n",
    "text_cols = ['categories', 'Covid Banner']\n",
    "\n",
    "from pyspark.sql.functions import when, count, col\n",
    "basetable.select([count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "           text_cols]).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing vavlues for the above 2 columns with -1\n",
    "basetable = basetable.fillna( { 'categories':-1 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer\n",
    "# Tokenizing\n",
    "tok_categories = Tokenizer(inputCol=\"categories\", outputCol=\"categories_words\")\n",
    "tok_covid_banner = Tokenizer(inputCol=\"Covid Banner\", outputCol=\"covid_banner_words\")\n",
    "\n",
    "# Note We do not need to remove stop words in our case\n",
    "\n",
    "# # Using Hashing Method\n",
    "# hashingTF_categories = HashingTF(inputCol=\"categories_words\", outputCol=\"categories_txt\")\n",
    "# hashingTF_covid_banner = HashingTF(inputCol=\"covid_banner_words\", outputCol=\"covid_banner_txt\")\n",
    "\n",
    "# Using CountVectorizer method\n",
    "cv_categories = CountVectorizer(inputCol=\"categories_words\", outputCol=\"categories_txt\")\n",
    "cv_covid_banner = CountVectorizer(inputCol=\"covid_banner_words\", outputCol=\"covid_banner_txt\")\n",
    "\n",
    "# Defining the pipeline\n",
    "pipeline = Pipeline(stages=[tok_categories, cv_categories, tok_covid_banner, cv_covid_banner])\n",
    "\n",
    "#Fit the pipeline to training documents.\n",
    "basetable = pipeline.fit(basetable).transform(basetable)\n",
    "\n",
    "# Dropping the columns\n",
    "basetable = basetable.drop(\"categories\", \"categories_words\", \"Covid Banner\", \"covid_banner_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+----------------+\n|      categories_txt|covid_banner_txt|\n+--------------------+----------------+\n|(1777,[0,84,94,16...|(5795,[0],[1.0])|\n|(1777,[0,1,4,14,2...|(5795,[0],[1.0])|\n|(1777,[2,3,96,266...|(5795,[0],[1.0])|\n+--------------------+----------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# confirming the above changes\n",
    "basetable.select(\"categories_txt\", \"covid_banner_txt\").show(3)"
   ]
  },
  {
   "source": [
    "### Creating a backup of the final basetable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming few columns from the basetable to export to parquet file as backup\n",
    "basetable = basetable.withColumnRenamed('avg(review_cool)','avg_review_cool')\n",
    "basetable = basetable.withColumnRenamed('avg(review_funny)','avg_review_funny')\n",
    "basetable = basetable.withColumnRenamed('avg(stars)','avg_stars')\n",
    "basetable = basetable.withColumnRenamed('avg(review_useful)','avg_review_useful')\n",
    "basetable = basetable.withColumnRenamed('avg(average_stars)','avg_average_stars')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_cool)','avg_compliment_cool')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_cute)','avg_compliment_cute')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_funny)','avg_compliment_funny')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_hot)','avg_compliment_hot')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_list)','avg_compliment_list')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_more)','avg_compliment_more')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_note)','avg_compliment_note')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_photos)','avg_compliment_photos')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_plain)','avg_compliment_plain')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_profile)','avg_compliment_profile')\n",
    "basetable = basetable.withColumnRenamed('avg(compliment_writer)','avg_compliment_writer')\n",
    "basetable = basetable.withColumnRenamed('avg(cool)','avg_cool')\n",
    "basetable = basetable.withColumnRenamed('avg(fans)','avg_fans')\n",
    "basetable = basetable.withColumnRenamed('avg(funny)','avg_funny')\n",
    "basetable = basetable.withColumnRenamed('avg(review_count)','avg_review_count')\n",
    "basetable = basetable.withColumnRenamed('avg(useful)','avg_useful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable = basetable.withColumnRenamed('Call To Action enabled_dum','Call_To_Action_enabled_dum')\n",
    "basetable = basetable.withColumnRenamed('Grubhub enabled_dum','Grubhub_enabled_dum')\n",
    "basetable = basetable.withColumnRenamed('Request a Quote Enabled_dum','Request_a_Quote_Enabled_dum')\n",
    "basetable = basetable.withColumnRenamed('Virtual Services Offered_dum','Virtual_Services_Offered_dum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the yelp_datamart dataframe to csv\n",
    "basetable.repartition(1).write.format(\"parquet\")\\\n",
    "  .mode('overwrite')\\\n",
    "  .option(\"header\", True)\\\n",
    "  .save(\"/FileStore/tables/Yelp_Data_Group/basetable_final_withOHE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 48)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the final basetable\n",
    "print((basetable.count(), len(basetable.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "basetable_final = basetable"
   ]
  },
  {
   "source": [
    "# Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the backup file\n",
    "# Reloading and checking\n",
    "basetable_final = spark.read.format(\"parquet\").load(\"/FileStore/tables/Yelp_Data_Group/basetable_final_withOHE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(19018, 48)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the loaded basetable\n",
    "print((basetable_final.count(), len(basetable_final.columns)))"
   ]
  },
  {
   "source": [
    "### Deleting highly correlated columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that are highly correlated and are \"hacking\" the AUC\n",
    "basetable_final = basetable_final.drop('attributes_RestaurantsDelivery_dum', 'attributes_RestaurantsTakeOut_dum', 'Grubhub_enabled_dum')"
   ]
  },
  {
   "source": [
    "### Basetable transformation for modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "basetable_final dataset no. of obs: 19018\n"
     ]
    }
   ],
   "source": [
    "#Transform the tables in a table of label, features format using the RFormula method\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "basetable_final = RFormula(formula=\"label ~ . - business_id\").fit(basetable_final).transform(basetable_final)\n",
    "\n",
    "print(\"basetable_final dataset no. of obs: \" + str(basetable_final.count()))"
   ]
  },
  {
   "source": [
    "### Creating a Train and Test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13280\n",
      "5738\n"
     ]
    }
   ],
   "source": [
    "#Create a train and test set with a 70% train, 30% test split\n",
    "train, test = basetable_final.randomSplit([0.7, 0.3],seed=123)\n",
    "\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|(7621,[0,1,2,3,4,...|  0.0|\n|(7621,[0,1,4,5,6,...|  0.0|\n|(7621,[0,1,3,4,5,...|  0.0|\n+--------------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Selecting only the features and the label columns for modeling\n",
    "train = train.select('features', 'label')\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|(7621,[0,1,4,6,7,...|  1.0|\n|(7621,[0,1,2,3,4,...|  1.0|\n|(7621,[0,1,2,3,4,...|  0.0|\n+--------------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Selecting only the features and the label columns for modeling\n",
    "test = test.select('features', 'label')\n",
    "test.show(3)"
   ]
  },
  {
   "source": [
    "## Modeling using Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for pipeline logistic regression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#Define pipeline\n",
    "rf = RandomForestClassifier(maxBins=5000)\n",
    "pipe_rf = Pipeline().setStages([rf])\n",
    "\n",
    "#Set param grid\n",
    "rf_params = ParamGridBuilder()\\\n",
    "  .addGrid(rf.numTrees, [150, 300, 500])\\\n",
    "  .build()\n",
    "\n",
    "# Evaluator: uses the max(AUC) by default for final model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Cross-validation of entire pipeline\n",
    "cv_rf = CrossValidator()\\\n",
    "  .setEstimator(pipe_rf)\\\n",
    "  .setEstimatorParamMaps(rf_params)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setNumFolds(10) # 10-fold cross validation\n",
    "\n",
    "#Run cross-validation. Spark automatically saves the best solution as the main model.\n",
    "cv_rf_model = cv_rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "0.8196477215125482\n",
      "0.8352864757192904\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Performing the predictionon the test set and subsetting data to just prediction and label columns\n",
    "preds = cv_rf_model.transform(test).select(\"prediction\", \"label\")\n",
    "preds.show(10)\n",
    "\n",
    "#Get model performance on test set\n",
    "out = preds.rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = BinaryClassificationMetrics(out)\n",
    "\n",
    "print(metrics.areaUnderPR) # area under precision/recall curve\n",
    "print(metrics.areaUnderROC) # area under Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Error = 0.119728 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(preds)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_724ee02eb965"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the train data\n",
    "\n",
    "display(cv_rf_model, train, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_724ee02eb965"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the test data\n",
    "\n",
    "display(cv_rf_model, test, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Calculating Feature Importance using Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=20, maxDepth=30, labelCol=\"label\", seed=42, maxBins=5000)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for extracing features \n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features_list = ExtractFeatureImp(model.featureImportances, predictions, rf.getFeaturesCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      idx                                        name  \\\n",
       "7599   27                             postal_code_Ind   \n",
       "7609   37                     attributes_HasTV_dum_-1   \n",
       "7614   42   attributes_RestaurantsGoodForGroups_dum_1   \n",
       "7607   35               attributes_GoodForKids_dum_-1   \n",
       "7610   38                      attributes_HasTV_dum_1   \n",
       "7613   41  attributes_RestaurantsGoodForGroups_dum_-1   \n",
       "7615   43   attributes_RestaurantsReservations_dum_-1   \n",
       "27     51                            categories_txt_2   \n",
       "28     52                            categories_txt_3   \n",
       "7616   44    attributes_RestaurantsReservations_dum_0   \n",
       "\n",
       "                                                   vals     score  \n",
       "7599  [89109, 85251, 89119, 85260, 85281, 89102, 891...  0.074763  \n",
       "7609                                                NaN  0.068454  \n",
       "7614                                                NaN  0.060426  \n",
       "7607                                                NaN  0.048551  \n",
       "7610                                                NaN  0.042574  \n",
       "7613                                                NaN  0.041207  \n",
       "7615                                                NaN  0.041174  \n",
       "27                                                  NaN  0.039653  \n",
       "28                                                  NaN  0.025071  \n",
       "7616                                                NaN  0.021813  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>idx</th>\n      <th>name</th>\n      <th>vals</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7599</th>\n      <td>27</td>\n      <td>postal_code_Ind</td>\n      <td>[89109, 85251, 89119, 85260, 85281, 89102, 891...</td>\n      <td>0.074763</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>37</td>\n      <td>attributes_HasTV_dum_-1</td>\n      <td>NaN</td>\n      <td>0.068454</td>\n    </tr>\n    <tr>\n      <th>7614</th>\n      <td>42</td>\n      <td>attributes_RestaurantsGoodForGroups_dum_1</td>\n      <td>NaN</td>\n      <td>0.060426</td>\n    </tr>\n    <tr>\n      <th>7607</th>\n      <td>35</td>\n      <td>attributes_GoodForKids_dum_-1</td>\n      <td>NaN</td>\n      <td>0.048551</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>38</td>\n      <td>attributes_HasTV_dum_1</td>\n      <td>NaN</td>\n      <td>0.042574</td>\n    </tr>\n    <tr>\n      <th>7613</th>\n      <td>41</td>\n      <td>attributes_RestaurantsGoodForGroups_dum_-1</td>\n      <td>NaN</td>\n      <td>0.041207</td>\n    </tr>\n    <tr>\n      <th>7615</th>\n      <td>43</td>\n      <td>attributes_RestaurantsReservations_dum_-1</td>\n      <td>NaN</td>\n      <td>0.041174</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>51</td>\n      <td>categories_txt_2</td>\n      <td>NaN</td>\n      <td>0.039653</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>52</td>\n      <td>categories_txt_3</td>\n      <td>NaN</td>\n      <td>0.025071</td>\n    </tr>\n    <tr>\n      <th>7616</th>\n      <td>44</td>\n      <td>attributes_RestaurantsReservations_dum_0</td>\n      <td>NaN</td>\n      <td>0.021813</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "features_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Modeling with Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for pipeline logistic regression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Define pipeline\n",
    "lr = LogisticRegression()\n",
    "pipe_lr = Pipeline().setStages([lr])\n",
    "\n",
    "#Set param grid\n",
    "lr_params = ParamGridBuilder()\\\n",
    "  .addGrid(lr.regParam, [0.1, 0.01])\\\n",
    "  .addGrid(lr.maxIter, [50, 100,150])\\\n",
    "  .build()\n",
    "\n",
    "# Evaluator: uses the max(AUC) by default for final model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Cross-validation of entire pipeline\n",
    "cv_lr = CrossValidator()\\\n",
    "  .setEstimator(pipe_lr)\\\n",
    "  .setEstimatorParamMaps(lr_params)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setNumFolds(10) # 10-fold cross validation\n",
    "\n",
    "#Run cross-validation. Spark automatically saves the best solution as the main model.\n",
    "cv_lr_model = cv_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "0.871934101798624\n",
      "0.9245368545335323\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Performing the predictionon the test set and subsetting data to just prediction and label columns\n",
    "preds = cv_lr_model.transform(test).select(\"prediction\", \"label\")\n",
    "preds.show(10)\n",
    "\n",
    "#Get model performance on test set\n",
    "out = preds.rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = BinaryClassificationMetrics(out)\n",
    "\n",
    "print(metrics.areaUnderPR) # area under precision/recall curve\n",
    "print(metrics.areaUnderROC) # area under Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Error = 0.0655281 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(preds)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_e79270ff480b"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the train data\n",
    "\n",
    "display(cv_lr_model, train, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_e79270ff480b"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the test data\n",
    "\n",
    "display(cv_lr_model, test, \"ROC\")"
   ]
  },
  {
   "source": [
    "## Modeling with Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for pipeline Decision Tree\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "#Define pipeline\n",
    "dt = DecisionTreeClassifier(maxBins=5000)\n",
    "pipe_dt = Pipeline().setStages([dt])\n",
    "\n",
    "#Set param grid\n",
    "dt_params = ParamGridBuilder()\\\n",
    "  .addGrid(dt.maxDepth, [10, 20, 30])\\\n",
    "  .build()\n",
    "\n",
    "# Evaluator: uses the max(AUC) by default for final model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Cross-validation of entire pipeline\n",
    "cv_dt = CrossValidator()\\\n",
    "  .setEstimator(pipe_dt)\\\n",
    "  .setEstimatorParamMaps(dt_params)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setNumFolds(10) #10-fold cross validation\n",
    "\n",
    "#Run cross-validation. Spark automatically saves the best solution as the main model.\n",
    "cv_dt_model = cv_dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "0.6452999518407262\n",
      "0.8393633900578056\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Performing the predictionon the test set and subsetting data to just prediction and label columns\n",
    "preds = cv_dt_model.transform(test).select(\"prediction\", \"label\")\n",
    "preds.show(10)\n",
    "\n",
    "#Get model performance on test set\n",
    "out = preds.rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = BinaryClassificationMetrics(out)\n",
    "\n",
    "print(metrics.areaUnderPR) # area under precision/recall curve\n",
    "print(metrics.areaUnderROC) # area under Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Error = 0.181945 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(preds)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_c42dba089b5d"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the train data\n",
    "\n",
    "display(cv_dt_model, train, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_c42dba089b5d"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the test data\n",
    "\n",
    "display(cv_dt_model, test, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Modeling with Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for pipeline Decision Tree\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "#Define pipeline\n",
    "gbt = GBTClassifier(maxBins=5000)\n",
    "pipe_gbt = Pipeline().setStages([gbt])\n",
    "\n",
    "#Set param grid\n",
    "gbt_params = ParamGridBuilder()\\\n",
    "             .build()\n",
    "\n",
    "# gbt_params = ParamGridBuilder()\\\n",
    "#              .addGrid(gbt.maxDepth, [10, 20])\\\n",
    "#              .addGrid(gbt.maxIter, [50, 100])\\\n",
    "#              .build()\n",
    "\n",
    "\n",
    "# Evaluator: uses the max(AUC) by default for final model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Cross-validation of entire pipeline\n",
    "cv_gbt = CrossValidator()\\\n",
    "  .setEstimator(pipe_gbt)\\\n",
    "  .setEstimatorParamMaps(gbt_params)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setNumFolds(10) #10-fold cross validation\n",
    "\n",
    "\n",
    "\n",
    "#Run cross-validation. Spark automatically saves the best solution as the main model.\n",
    "cv_gbt_model = cv_gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "|       0.0|  0.0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "0.646784754500806\n",
      "0.8464686044595405\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Performing the predictionon the test set and subsetting data to just prediction and label columns\n",
    "preds = cv_gbt_model.transform(test).select(\"prediction\", \"label\")\n",
    "preds.show(10)\n",
    "\n",
    "#Get model performance on test set\n",
    "out = preds.rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = BinaryClassificationMetrics(out)\n",
    "\n",
    "print(metrics.areaUnderPR) # area under precision/recall curve\n",
    "print(metrics.areaUnderROC) # area under Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Error = 0.180028 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(preds)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_2203520bd7b9"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the train data\n",
    "\n",
    "display(cv_gbt_model, train, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "CrossValidatorModel_2203520bd7b9"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[features: vector, label: double]"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'ROC'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# ROC curve on the test data\n",
    "\n",
    "display(cv_gbt_model, test, \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}